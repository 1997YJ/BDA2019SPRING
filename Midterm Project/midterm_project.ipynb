{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6107eab87465>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import re as re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba\n",
    "import mpl_finance as mpf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, SelectKBest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection, metrics, ensemble\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import time\n",
    "import math\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_convert_dtype(stock_pd):\n",
    "    stock_pd['年月日'] = pd.to_datetime(stock_pd['年月日'])\n",
    "    stock_pd['開盤價(元)']= stock_pd['開盤價(元)'].apply(lambda x: x.replace(',', '')).astype('float')\n",
    "    stock_pd['最高價(元)']= stock_pd['最高價(元)'].apply(lambda x: x.replace(',', '')).astype('float')\n",
    "    stock_pd['最低價(元)']= stock_pd['最低價(元)'].apply(lambda x: x.replace(',', '')).astype('float')\n",
    "    stock_pd['收盤價(元)']= stock_pd['收盤價(元)'].apply(lambda x: x.replace(',', '')).astype('float')\n",
    "    stock_pd['成交量(千股)']= stock_pd['成交量(千股)'].apply(lambda x: x.replace(',', '')).astype('int')\n",
    "    stock_pd['成交值(千元)']= stock_pd['成交值(千元)'].apply(lambda x: x.replace(',', '')).astype('int')\n",
    "    stock_pd['成交筆數(筆)']= stock_pd['成交筆數(筆)'].apply(lambda x: x.replace(',', '')).astype('int')\n",
    "    stock_pd['流通在外股數(千股)']= stock_pd['流通在外股數(千股)'].apply(lambda x: x.replace(',', '')).astype('int')\n",
    "    stock_pd['本益比-TSE'] = stock_pd['本益比-TSE'].astype('str')\n",
    "    stock_pd['本益比-TSE']= stock_pd['本益比-TSE'].apply(lambda x: x.replace(',', '')).astype('float')\n",
    "    return stock_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hdr_idx(file_pd):\n",
    "    file_idx2hdr = list(file_pd)\n",
    "    file_hdr2idx = dict((hdr, idx) for idx, hdr in enumerate(file_idx2hdr))\n",
    "    return file_idx2hdr, file_hdr2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_2018_pd = pd.read_csv('bda2019_dataset/listed_company_2018.csv')\n",
    "listed_2017_pd = pd.read_csv('bda2019_dataset/listed_company_2017.csv')\n",
    "listed_2016_pd = pd.read_csv('bda2019_dataset/listed_company_2016.csv')\n",
    "listed_2016_pd = stock_convert_dtype(listed_2016_pd)\n",
    "listed_2017_pd = stock_convert_dtype(listed_2017_pd)\n",
    "listed_2018_pd = stock_convert_dtype(listed_2018_pd)\n",
    "stocks_pd = listed_2018_pd.append(listed_2017_pd, ignore_index=True)\n",
    "stocks_pd = stocks_pd.append(listed_2016_pd, ignore_index=True)\n",
    "stocks_idx2hdr, stocks_hdr2idx = get_file_hdr_idx(stocks_pd)\n",
    "# stocks_pd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter_2018_pd = pd.read_csv('bda2019_dataset/counter_company_2018.csv')\n",
    "# counter_2017_pd = pd.read_csv('bda2019_dataset/counter_company_2017.csv')\n",
    "# counter_2016_pd = pd.read_csv('bda2019_dataset/counter_company_2016.csv')\n",
    "# counter_2016_pd = stock_convert_dtype(listed_2016_pd)\n",
    "# counter_2017_pd = stock_convert_dtype(listed_2017_pd)\n",
    "# counter_2018_pd = stock_convert_dtype(listed_2018_pd)\n",
    "# stocks_pd = stocks_pd.append(counter_2018_pd, ignore_index=True)\n",
    "# stocks_pd = stocks_pd.append(counter_2017_pd, ignore_index=True)\n",
    "# stocks_pd = stocks_pd.append(counter_2016_pd, ignore_index=True)\n",
    "# stocks_idx2hdr, stocks_hdr2idx = get_file_hdr_idx(stocks_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_pd = pd.read_csv('bda2019_dataset/news.csv',encoding='big5')\n",
    "news_pd['post_time'] = pd.to_datetime(news_pd['post_time'])\n",
    "news_pd['content'] = news_pd['content'].astype('str')\n",
    "news_idx2hdr, news_hdr2idx = get_file_hdr_idx(news_pd)\n",
    "# news_pd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbs_pd = pd.read_csv('bda2019_dataset/bbs-utf8.csv')\n",
    "bbs_pd['post_time'] = pd.to_datetime(bbs_pd['post_time'])\n",
    "bbs_pd['content'] = bbs_pd['content'].astype('str')\n",
    "bbs_idx2hdr, bbs_hdr2idx = get_file_hdr_idx(bbs_pd)\n",
    "# bbs_pd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_pd = pd.read_csv('bda2019_dataset/forum-utf8.csv')\n",
    "forum_pd['post_time'] = pd.to_datetime(forum_pd['post_time'])\n",
    "forum_pd['content'] = forum_pd['content'].astype('str')\n",
    "forum_idx2hdr, forum_hdr2idx = get_file_hdr_idx(forum_pd)\n",
    "# forum_pd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_pd = pd.read_csv('權值股.csv')\n",
    "weight_idx2hdr, weight_hdr2idx = get_file_hdr_idx(weight_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converts to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_np = stocks_pd.values\n",
    "stocks_np = stocks_np[::-1] # reverse\n",
    "news_np = news_pd.values\n",
    "bbs_np = bbs_pd.values\n",
    "forum_np = forum_pd.values\n",
    "weight_np = weight_pd.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_companys(company, stock_np, name_index=0):\n",
    "    indexs = []\n",
    "    for i in range(stock_np.shape[0]):\n",
    "        if company in stock_np[i][name_index]:\n",
    "            indexs.append(i)\n",
    "    new_np = stock_np[indexs,:]\n",
    "#     print('Company Shape = ', new_np.shape)\n",
    "    return new_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_by_all_keyword(keyword_list, org_array_np, from_index=0):\n",
    "    indexs = []\n",
    "    for i in range(org_array_np.shape[0]):\n",
    "        if all(keyword in org_array_np[i][from_index] for keyword in keyword_list):\n",
    "            indexs.append(i)\n",
    "    new_array_np = org_array_np[indexs,:]\n",
    "#     print('New array Shape = ', new_ar?ray_np.shape)\n",
    "    return new_array_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_by_any_keyword(keyword_list, org_array_np, from_index=0):\n",
    "    indexs = []\n",
    "    for i in range(org_array_np.shape[0]):\n",
    "        if any(keyword in org_array_np[i][from_index] for keyword in keyword_list):\n",
    "            indexs.append(i)\n",
    "    new_array_np = org_array_np[indexs,:]\n",
    "#     print('New array Shape = ', new_array_np.shape)\n",
    "    return new_array_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ma(price, avg_num):\n",
    "    ones = [1] * avg_num\n",
    "    ma = np.convolve(price, ones)\n",
    "    ma = ma[0:len(price)]/avg_num\n",
    "    return ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_by_2line(line1, line2):\n",
    "    line_diff = line1-line2\n",
    "    line1_greater = np.where(line_diff>0, 1, 0)\n",
    "    line1_greater_org = np.insert(line1_greater, len(line1_greater)-1, 0)\n",
    "    line1_greater_dly = np.insert(line1_greater, 0, 0)\n",
    "    line1_lead = line1_greater_org & ~line1_greater_dly\n",
    "    line2_lead = ~line1_greater_org & line1_greater_dly\n",
    "    line1_lead_idx = np.where(line1_lead>0)[0]\n",
    "    line2_lead_idx = np.where(line2_lead>0)[0]\n",
    "    return line1_lead_idx, line2_lead_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_idx(up_idx, down_idx, up_thr, down_thr, stock_np, close_price_idx):\n",
    "    up_idx = up_idx[(up_idx>3) & (up_idx<len(company_stock_np)-3)]\n",
    "    down_idx = down_idx[(down_idx>3) & (down_idx<len(company_stock_np)-3)]\n",
    "    post1 = company_stock_np[up_idx+1,close_price_idx]\n",
    "    post2 = company_stock_np[up_idx+2,close_price_idx]\n",
    "    post3 = company_stock_np[up_idx+3,close_price_idx]\n",
    "    pre1 = company_stock_np[up_idx-1,close_price_idx]\n",
    "    pre2 = company_stock_np[up_idx-2,close_price_idx]\n",
    "    pre3 = company_stock_np[up_idx-3,close_price_idx]\n",
    "    up_amp = (post1+post2+post3)/(pre1+pre2+pre3)\n",
    "    up_vld_idx = up_idx[up_amp>up_thr]\n",
    "    post1 = company_stock_np[down_idx+1,close_price_idx]\n",
    "    post2 = company_stock_np[down_idx+2,close_price_idx]\n",
    "    post3 = company_stock_np[down_idx+3,close_price_idx]\n",
    "    pre1 = company_stock_np[down_idx-1,close_price_idx]\n",
    "    pre2 = company_stock_np[down_idx-2,close_price_idx]\n",
    "    pre3 = company_stock_np[down_idx-3,close_price_idx]\n",
    "    down_amp = (pre1+pre2+pre3)/(post1+post2+post3)\n",
    "    down_vld_idx = down_idx[down_amp>down_thr]\n",
    "    return up_vld_idx, down_vld_idx, up_amp, down_amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hdr_idx(file_pd):\n",
    "    file_idx2hdr = list(file_pd)\n",
    "    file_hdr2idx = dict((hdr, idx) for idx, hdr in enumerate(file_idx2hdr))\n",
    "    return file_idx2hdr, file_hdr2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates_by_idx(idx, stock_np, date_idx, pre_date):\n",
    "    try:\n",
    "        dates = stock_np[idx-pre_date, date_idx]\n",
    "    except:\n",
    "        dates = []\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ma_up_dn_dates(company_stock_np,stocks_hdr2idx,thr_up,thr_lo,pre_day):\n",
    "    close_price = company_stock_np[:,stocks_hdr2idx['收盤價(元)']]\n",
    "    ma20 = get_ma(close_price, 20)\n",
    "    ma5 = get_ma(close_price, 5)\n",
    "    up_idx, dn_idx = get_idx_by_2line(ma5, ma20)\n",
    "    up_vld_idx, dn_vld_idx, up_amp, dn_amp = get_valid_idx(\n",
    "        up_idx, dn_idx, thr_up, thr_lo, company_stock_np, stocks_hdr2idx['收盤價(元)'])\n",
    "    up_dates = get_dates_by_idx(up_vld_idx, company_stock_np, stocks_hdr2idx['年月日'], pre_day)\n",
    "    dn_dates = get_dates_by_idx(dn_vld_idx, company_stock_np, stocks_hdr2idx['年月日'], pre_day)\n",
    "    return up_dates, dn_dates, ma20, ma5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kd(close_price):\n",
    "    # Lowest price in past 9 days\n",
    "    low_price_matrix = np.ones(shape = (9,len(low_price)+8))*10000\n",
    "    for n in range(0,9):\n",
    "        low_price_matrix[n,n:len(low_price)+n] = low_price\n",
    "    low_price_pass9 = np.amin(low_price_matrix, axis=0)\n",
    "    low_price_pass9 = low_price_pass9[0:len(low_price)]\n",
    "    # Highest price in past 9 days\n",
    "    high_price_matrix = np.zeros(shape = (9,len(high_price)+8))\n",
    "    for n in range(0,9):\n",
    "        high_price_matrix[n,n:len(high_price)+n] = high_price\n",
    "    high_price_pass9 = np.amax(high_price_matrix, axis=0)\n",
    "    high_price_pass9 = high_price_pass9[0:len(high_price)]\n",
    "    # rsv\n",
    "    sp_idx = np.where(high_price_pass9==low_price_pass9) # special case\n",
    "    low_price_pass9[sp_idx] = low_price_pass9[sp_idx]-1\n",
    "    rsv = (close_price-low_price_pass9)/(high_price_pass9-low_price_pass9)\n",
    "    # KD\n",
    "    kd_k = 1/3*rsv\n",
    "    kd_d = 1/3*rsv\n",
    "    for n in range(1,len(rsv)):\n",
    "        kd_k[n] = 2/3*kd_k[n-1] + 1/3*rsv[n]\n",
    "        kd_d[n] = 2/3*kd_d[n-1] + 1/3*kd_k[n]\n",
    "    return kd_k, kd_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kd_up_dn_dates(company_stock_np,stocks_hdr2idx,thr,pre_day):\n",
    "    close_price = company_stock_np[:,stocks_hdr2idx['收盤價(元)']]\n",
    "    kd_k, kd_d = get_kd(close_price)\n",
    "    up_idx, dn_idx = get_idx_by_2line(kd_k, kd_d)\n",
    "    up_vld_idx, dn_vld_idx, up_amp, dn_amp = get_valid_idx(\n",
    "        up_idx, dn_idx, thr, thr, company_stock_np, stocks_hdr2idx['收盤價(元)'])\n",
    "    up_dates = get_dates_by_idx(up_vld_idx, company_stock_np, stocks_hdr2idx['年月日'], pre_day)\n",
    "    dn_dates = get_dates_by_idx(dn_vld_idx, company_stock_np, stocks_hdr2idx['年月日'], pre_day)\n",
    "    return up_dates, dn_dates, kd_k, kd_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vol_up_dn_dates(company_stock_np,stocks_hdr2idx,ranks_per_year,pre_day):\n",
    "    close_price = company_stock_np[:,stocks_hdr2idx['收盤價(元)']]\n",
    "    volumn = company_stock_np[:,stocks_hdr2idx['成交筆數(筆)']]\n",
    "    open_price = company_stock_np[:,stocks_hdr2idx['開盤價(元)']]\n",
    "    close_price = company_stock_np[:,stocks_hdr2idx['收盤價(元)']]\n",
    "    up_idx = []\n",
    "    dn_idx = []\n",
    "    for year in range(2016,2019):\n",
    "        year_idx = np.where( (company_stock_np[:,stocks_hdr2idx['年月日']]>=datetime(year  , 1, 1, 0, 0))\n",
    "                           & (company_stock_np[:,stocks_hdr2idx['年月日']]< datetime(year+1, 1, 1, 0, 0))\n",
    "                           )\n",
    "        try:\n",
    "            init_idx = year_idx[0][0]\n",
    "        except:\n",
    "            init_idx = 0\n",
    "        top_vol_idx = np.argsort(volumn[year_idx])[::-1][0:ranks_per_year]+init_idx\n",
    "        dn_vol = open_price[top_vol_idx]>close_price[top_vol_idx]\n",
    "        for n in range(0,len(dn_vol)):\n",
    "            if dn_vol[n]:\n",
    "                dn_idx.append(top_vol_idx[n])\n",
    "            else:\n",
    "                up_idx.append(top_vol_idx[n])\n",
    "    up_dates = get_dates_by_idx(np.array(up_idx), company_stock_np, stocks_hdr2idx['年月日'], pre_day)\n",
    "    dn_dates = get_dates_by_idx(np.array(dn_idx), company_stock_np, stocks_hdr2idx['年月日'], pre_day)\n",
    "    return up_dates, dn_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_by_date(dates, data, hdr2idx):\n",
    "    docs = []\n",
    "    date_index = hdr2idx['post_time']\n",
    "    title_index = hdr2idx['title']\n",
    "    content_index = hdr2idx['content']\n",
    "    author_indx = hdr2idx['author']\n",
    "    for row in data:\n",
    "        for date in dates:\n",
    "            if row[date_index].date() == date.date():\n",
    "                temp_doc = '%s %s %s'%(row[title_index],row[title_index],row[content_index])\n",
    "                docs.append(remove_notation(temp_doc))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_cnt_by_date(dates, data, hdr2idx):\n",
    "    doc_cnt = []\n",
    "    date_index = hdr2idx['post_time']\n",
    "    title_index = hdr2idx['title']\n",
    "    content_index = hdr2idx['content']\n",
    "    author_indx = hdr2idx['author']\n",
    "    for date in dates:\n",
    "        doc_cnt_tmp = 0\n",
    "        for row in data:\n",
    "            if row[date_index].date() == date.date():\n",
    "                doc_cnt_tmp += 1\n",
    "        doc_cnt.append(doc_cnt_tmp)\n",
    "    return doc_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_notation(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<br>',' ',text)\n",
    "    text = re.sub('http[\\a-z0-9]* ',' ',text)\n",
    "    text = re.sub('\\d+',' ',text) # remove number\n",
    "    text = re.sub('[^\\w\\s]',' ',text) # remove punctuation\n",
    "    text = re.sub(r'\\b\\w\\b',' ',text) # remove single eng character\n",
    "    text = re.sub(r'[\\u4e00-\\u9fa5][a-z]\\b','',text)\n",
    "    text = re.sub(r'\\b[a-z][\\u4e00-\\u9fa5][a-z]','',text)\n",
    "    text = re.sub('\\s+',' ',text) # combine space\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctgry_doc_by_date(company, ctgry_np, ctgry_hdr2idx, up_dates, dn_dates):\n",
    "    ctgry = ctgry_np[0,1]\n",
    "    company_np = get_companys(company=company, stock_np=ctgry_np, name_index=ctgry_hdr2idx['content'])\n",
    "    up_docs = np.array(get_doc_by_date(list(up_dates), company_np, ctgry_hdr2idx) )\n",
    "    dn_docs = np.array(get_doc_by_date(list(dn_dates), company_np, ctgry_hdr2idx) )\n",
    "    return up_docs, dn_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctgry_doc_by_allkeyword_date(keyword_list, ctgry_np, ctgry_hdr2idx, up_dates, dn_dates):\n",
    "    ctgry = ctgry_np[0,1]\n",
    "    kw_np = get_by_all_keyword(keyword_list=keyword_list, org_array_np=ctgry_np, from_index=ctgry_hdr2idx['content'])\n",
    "    up_docs = np.array(get_doc_by_date(list(up_dates), kw_np, ctgry_hdr2idx) )\n",
    "    dn_docs = np.array(get_doc_by_date(list(dn_dates), kw_np,ctgry_hdr2idx) )\n",
    "    up_date_doc_cnt = get_doc_cnt_by_date(list(up_dates), ctgry_np, ctgry_hdr2idx)\n",
    "    dn_date_doc_cnt = get_doc_cnt_by_date(list(dn_dates), ctgry_np, ctgry_hdr2idx)\n",
    "\n",
    "    print('{}: There are {} up docs and {} down {} docs'.format(company, len(up_docs), len(dn_docs), ctgry.upper()))\n",
    "    return up_docs, dn_docs, up_date_doc_cnt, dn_date_doc_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctgry_doc_by_anykeyword_date(keyword_list, ctgry_np, ctgry_hdr2idx, up_dates, dn_dates):\n",
    "    ctgry = ctgry_np[0,1]\n",
    "    \n",
    "    kw_np = get_by_any_keyword(keyword_list, ctgry_np, ctgry_hdr2idx['content'])\n",
    "    print(len(kw_np), len(ctgry_np))\n",
    "    print(keyword_list)\n",
    "    up_docs = np.array(get_doc_by_date(list(up_dates), kw_np, ctgry_hdr2idx) )\n",
    "    dn_docs = np.array(get_doc_by_date(list(dn_dates), kw_np, ctgry_hdr2idx) )\n",
    "\n",
    "    print('{}: There are {} up docs and {} down {} docs'.format(company, len(up_docs), len(dn_docs), ctgry.upper()))\n",
    "    return up_docs, dn_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords(file_name):\n",
    "    stopwords = []\n",
    "    with open(file_name,'r', encoding='UTF-8') as f1:\n",
    "        for line in f1:\n",
    "            stopwords.append(line.strip())\n",
    "    stopwords.append('nan')\n",
    "    stopwords.append(' ')\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_tool(corpus):\n",
    "    vectorizer = CountVectorizer(tokenizer=jieba.cut, analyzer='word', min_df=2, stop_words=stopwords)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "#     print(X.shape)\n",
    "    return X, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_chi2(all_vectors, clas_vectors):\n",
    "    expected =all_vectors.sum(axis=0)*len(clas_vectors)/len(all_vectors)\n",
    "    clas_value = clas_vectors.sum(axis=0)\n",
    "    sign_idx = np.where(clas_value-expected < 0)[0]\n",
    "    chi2_value = np.power(clas_value-expected,2)/expected\n",
    "    chi2_value[sign_idx] = chi2_value[sign_idx] * -1\n",
    "    return chi2_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(term_list, doc_list):\n",
    "    df_list = list()\n",
    "    for keyword in term_list:\n",
    "        df = 0\n",
    "        for text in doc_list:\n",
    "            if keyword in text:\n",
    "                df = df + 1\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(term_list, doc_list):\n",
    "    tf_list = list()\n",
    "    for text in doc_list:\n",
    "        tf_tmp = list()\n",
    "        for keyword in term_list:\n",
    "            tf_tmp.append(text.count(keyword)) # append tf for 1 doc\n",
    "        tf_list.append(tf_tmp) # append tf of doc to tf_list\n",
    "    tf_np = np.array(tf_list)\n",
    "    return tf_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc Selection (News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_list = ['刪除','請問','請教','自刪','繼承','信用卡','匯率','保單','遊戲','關閉']\n",
    "keyword_list = ['盤後','盤中','盤前','交易概況','上市認購','晨訊','各報要聞','報價簡訊','台北股市','海外存託憑證',\n",
    "                '國內匯市','證交所','y早報','y晚報','焦點新聞','投顧','晨間解析','集中市場']\n",
    "keyword_list_a = ['買進','賣出']\n",
    "keyword_list_b = ['上漲','下跌']\n",
    "keyword_list_c = ['跌破','衝上']\n",
    "keyword_list_d = ['買超','賣超']\n",
    "# ctgry_np = forum_np\n",
    "# ctgry_hdr2idx = forum_hdr2idx\n",
    "ctgry_np = news_np\n",
    "ctgry_hdr2idx = news_hdr2idx\n",
    "\n",
    "indexs = []\n",
    "ctgry_trunc_np = []\n",
    "for i in range(ctgry_np.shape[0]):\n",
    "    context = (ctgry_np[i,ctgry_hdr2idx['title']] + ctgry_np[i,ctgry_hdr2idx['content']]).lower()\n",
    "\n",
    "    if not (any(keyword in context for keyword in keyword_list) or\n",
    "        all(keyword_a in context for keyword_a in keyword_list_a) or\n",
    "        all(keyword_b in context for keyword_b in keyword_list_b) or \n",
    "        all(keyword_c in context for keyword_c in keyword_list_c) or\n",
    "        all(keyword_d in context for keyword_d in keyword_list_d) ):\n",
    "        indexs.append(i)\n",
    "ctgry_trunc_np = ctgry_np[indexs,:]\n",
    "\n",
    "print('Original {} docs, reduced to {} docs'.format(len(ctgry_np), len(ctgry_trunc_np)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramter List (Training sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "pre_day_t = 2\n",
    "pre_day_dn = pre_day_t\n",
    "test_cut_time = datetime(2018, 1, 1, 0, 0)\n",
    "up_tick = 1\n",
    "dn_tick = 0\n",
    "no_tick = 2\n",
    "sigma = 0.0\n",
    "sigma_dn = 0.0\n",
    "company_list= ['台積電']\n",
    "# company_list= ['中信金']\n",
    "# company_list= ['第一金']\n",
    "# company_list = ['台積電', '鴻海', '大立光', '南亞', '台塑', '華新', '國泰金', '聯發科']\n",
    "# company_list = ['台積電']\n",
    "# company_list = ['鴻海']\n",
    "# company_list = ['國泰金']\n",
    "# company_list = ['大立光']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramter List (Testing sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company = '國泰金'\n",
    "# company = '台積電'\n",
    "# test_cut_time = datetime(2018,7,1, 0, 0)\n",
    "pre_day = 3\n",
    "tcnt_ma_f_len = 5\n",
    "tcnt_ma_s_len = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Up & Down docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (time.asctime( time.localtime(time.time()) ))\n",
    "up_docs_all = []\n",
    "dn_docs_all = []\n",
    "# ctgry_np = ctgry_trunc_np\n",
    "# ctgry_hdr2_idx = forum_hdr2idx\n",
    "# ctgry_np = news_np\n",
    "# ctgry_hdr2idx = news_hdr2idx\n",
    "for company in company_list:\n",
    "    # Grep company stocks\n",
    "    company_stock_np = get_companys(company, stocks_np, stocks_hdr2idx['證券代碼'])\n",
    "    # Cut test set for future use\n",
    "    company_stock_train_np = company_stock_np[company_stock_np[:,1]<test_cut_time,:]\n",
    "    pe_ratio = company_stock_train_np[:,stocks_hdr2idx['本益比-TSE']]\n",
    "    pb_ratio = company_stock_train_np[:,stocks_hdr2idx['股價淨值比-TSE']]\n",
    "    open_price = company_stock_train_np[:,stocks_hdr2idx['開盤價(元)']]\n",
    "    high_price = company_stock_train_np[:,stocks_hdr2idx['最高價(元)']]\n",
    "    low_price = company_stock_train_np[:,stocks_hdr2idx['最低價(元)']]\n",
    "    close_price = company_stock_train_np[:,stocks_hdr2idx['收盤價(元)']]\n",
    "    volumn = company_stock_train_np[:,stocks_hdr2idx['成交筆數(筆)']]\n",
    "    \n",
    "    # up down ticks\n",
    "    close_price_org = np.insert(close_price, len(close_price)-1, close_price[-1])\n",
    "    close_price_dly = np.insert(close_price, 0, close_price[0])\n",
    "    for n in range(1,pre_day_t):\n",
    "        close_price_org = np.insert(close_price_org, len(close_price_org)-1, close_price_org[-1])\n",
    "        close_price_dly = np.insert(close_price_dly, n, close_price[n])\n",
    "    close_price_percent = (close_price_org-close_price_dly) / close_price_org\n",
    "    up_down = np.ones(len(close_price_org))*no_tick\n",
    "    up_down[np.where(close_price_percent> sigma)[0]] = up_tick\n",
    "    up_down[np.where(close_price_percent<-sigma_dn)[0]] = dn_tick\n",
    "\n",
    "    # MA, KD, Volumn\n",
    "#     ma_up_dates, ma_dn_dates, ma20, ma5 = get_ma_up_dn_dates(company_stock_train_np,stocks_hdr2idx,1.01,1.01,pre_day_t)\n",
    "#     kd_up_dates, kd_dn_dates, kd_k, kd_d = get_kd_up_dn_dates(company_stock_train_np,stocks_hdr2idx,1.05,pre_day_t)\n",
    "#     vol_up_dates, vol_dn_dates = get_vol_up_dn_dates(company_stock_train_np,stocks_hdr2idx,5,pre_day_t)\n",
    "#     up_dates = ma_up_dates\n",
    "#     dn_dates = ma_dn_dates\n",
    "    # up down dates\n",
    "    up_dates = (company_stock_np[np.where(up_down==up_tick),stocks_hdr2idx['年月日']] - timedelta(days=pre_day_t))[0]\n",
    "    dn_dates = (company_stock_np[np.where(up_down==dn_tick),stocks_hdr2idx['年月日']] - timedelta(days=pre_day_dn))[0]\n",
    "    \n",
    "    up_docs, dn_docs = get_ctgry_doc_by_date(company, ctgry_np, ctgry_hdr2idx, up_dates, dn_dates)\n",
    "\n",
    "    up_docs_all = np.concatenate((up_docs_all, up_docs))\n",
    "    dn_docs_all = np.concatenate((dn_docs_all, dn_docs))\n",
    "print('There are {} up docs, {} down docs'.format(len(up_docs_all), len(dn_docs_all)))\n",
    "print (time.asctime( time.localtime(time.time()) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_docs_all_new = up_docs_all\n",
    "dn_docs_all_new = dn_docs_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = get_stopwords('stopwords.txt')\n",
    "up_all_docs = up_docs_all_new\n",
    "dn_all_docs = dn_docs_all_new\n",
    "# up_all_docs = up_docs_all\n",
    "# dn_all_docs = dn_docs_all\n",
    "all_docs = np.append(up_all_docs  ,dn_all_docs)\n",
    "chi_all_x, chi_all_vec = get_count_tool(all_docs)\n",
    "y = np.append([up_tick]*len(up_all_docs),[dn_tick]*len(dn_all_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_all_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_all_matrix = chi_all_x.toarray()\n",
    "chi_up_matrix = chi_all_matrix[0:len(up_all_docs)]\n",
    "chi_down_matrix = chi_all_matrix[len(up_all_docs):len(chi_all_matrix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_up = my_chi2(chi_all_matrix, chi_up_matrix)\n",
    "chi2_down = my_chi2(chi_all_matrix, chi_down_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_indexs = np.argsort(chi2_up)[::-1]\n",
    "tokens = np.array(chi_all_vec.get_feature_names())\n",
    "chi2_up_features = tokens[max_indexs[0:feature_num]]\n",
    "# chi2_up_features = tokens[max_indexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_up_x = chi_all_matrix[0:len(up_all_docs),max_indexs[0:feature_num]]\n",
    "# chi2_up_x = chi_all_matrix[0:len(up_all_docs),max_indexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_up_features[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_indexs = np.argsort(chi2_down)[::-1]\n",
    "tokens = np.array(chi_all_vec.get_feature_names())\n",
    "chi2_down_features = tokens[max_indexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_dn_x = chi_all_matrix[len(up_all_docs):len(chi_all_matrix),max_indexs[0:feature_num]]\n",
    "# chi2_dn_x = chi_all_matrix[len(up_all_docs):len(chi_all_matrix),max_indexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_down_features[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_f_row = len(up_all_docs)+len(dn_all_docs)\n",
    "chi2_up_fs = list(chi2_up_features[0:feature_num]).copy()\n",
    "chi2_dn_fs = list(chi2_down_features[0:feature_num]).copy()\n",
    "# chi2_up_fs = list(chi2_up_features).copy()\n",
    "# chi2_dn_fs = list(chi2_down_features).copy()\n",
    "chi2_up_fs=[i for i in chi2_up_fs if i not in chi2_dn_fs]\n",
    "chi2_dn_fs=[i for i in chi2_dn_fs if i not in chi2_up_fs]\n",
    "chi2_fs = chi2_up_fs + chi2_dn_fs\n",
    "chi2_f_col = len(chi2_up_fs) + len(chi2_dn_fs)\n",
    "chi2_features = np.zeros((chi2_f_row, chi2_f_col))\n",
    "chi2_features[0:len(up_all_docs),0:len(chi2_up_fs)] = chi2_up_x[:,0:len(chi2_up_fs)]\n",
    "chi2_features[len(up_all_docs):chi2_f_row,len(chi2_up_fs):chi2_f_col] = chi2_dn_x[:,0:len(chi2_dn_fs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_x = chi2_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.append([up_tick]*len(up_all_docs),[dn_tick]*len(dn_all_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kw = chi2_fs\n",
    "# print(len(all_kw))\n",
    "print('There are {} features'.format(len(all_kw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=3, random_state=0, shuffle=False)\n",
    "print(kf)\n",
    "forest = ensemble.RandomForestClassifier(n_estimators = 100)\n",
    "# my_svm = SVC(gamma='auto')\n",
    "for train_index, test_index in kf.split(trans_x,y):\n",
    "    X_train, X_test = trans_x[train_index], trans_x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    forest_fit = forest.fit(X_train, y_train)\n",
    "    test_y_predicted = forest.predict(X_test)\n",
    "#     svc_fit = my_svm.fit(X_train, y_train)\n",
    "#     test_y_predicted = my_svm.predict(X_test)\n",
    "#     test_y_predicted = np.logical_not(test_y_predicted).astype(int)\n",
    "    target_name = ['Down', 'Up']\n",
    "    print(metrics.classification_report(y_test, test_y_predicted, target_names=target_name))\n",
    "    accuracy = metrics.accuracy_score(y_test, test_y_predicted)\n",
    "    print('Accuracy = ',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_stock_np = get_companys(company=company, stock_np=stocks_np, name_index=stocks_hdr2idx['證券代碼'])\n",
    "company_stock_test_np = company_stock_np[company_stock_np[:,1]>=test_cut_time,:]\n",
    "pe_ratio = company_stock_test_np[:,stocks_hdr2idx['本益比-TSE']]\n",
    "pb_ratio = company_stock_test_np[:,stocks_hdr2idx['股價淨值比-TSE']]\n",
    "open_price = company_stock_test_np[:,stocks_hdr2idx['開盤價(元)']]\n",
    "high_price = company_stock_test_np[:,stocks_hdr2idx['最高價(元)']]\n",
    "low_price = company_stock_test_np[:,stocks_hdr2idx['最低價(元)']]\n",
    "close_price = company_stock_test_np[:,stocks_hdr2idx['收盤價(元)']]\n",
    "volumn = company_stock_test_np[:,stocks_hdr2idx['成交筆數(筆)']]\n",
    "\n",
    "close_price_org = np.insert(close_price, len(close_price)-1, close_price[-1])\n",
    "close_price_dly = np.insert(close_price, 0, close_price[0])\n",
    "for n in range(1,pre_day):\n",
    "    close_price_org = np.insert(close_price_org, len(close_price_org)-1, close_price_org[-1])\n",
    "    close_price_dly = np.insert(close_price_dly, n, close_price[n])\n",
    "\n",
    "up_down = np.ones(len(close_price_org))*no_tick\n",
    "up_down = np.where(close_price_org>close_price_dly,up_tick,up_down)\n",
    "up_down = np.where(close_price_org<close_price_dly,dn_tick,up_down)\n",
    "\n",
    "company_ctgry_np = get_companys(company, ctgry_trunc_np, ctgry_hdr2idx['content'])\n",
    "# company_ctgry_np = get_companys(company, news_np, news_hdr2idx['content'])\n",
    "# company_forum_np = get_by_any_keyword(keyword_list, forum_np, forum_hdr2idx['content'])\n",
    "# ctgry_hdr2idx = forum_hdr2idx\n",
    "predict_y = []\n",
    "actual_y = []\n",
    "ttl_doc_cnt = []\n",
    "up_doc_cnt = []\n",
    "dn_doc_cnt = []\n",
    "open_price_array = []\n",
    "close_price_array = []\n",
    "high_price_array = []\n",
    "low_price_array = []\n",
    "test_day_array = []\n",
    "p_up_down = 0 # 1st guess\n",
    "ttl_doc_ma_f = list(np.zeros(tcnt_ma_f_len))\n",
    "ttl_doc_ma_s = list(np.zeros(tcnt_ma_s_len))\n",
    "ttl_ma_cross = 1\n",
    "for test_day in range(0,len(company_stock_test_np)-10):\n",
    "\n",
    "    today_date = company_stock_test_np[test_day:test_day+1,stocks_hdr2idx['年月日']]\n",
    "    \n",
    "    future_idx = test_day+pre_day\n",
    "    open_price_array.append(list(company_stock_test_np[future_idx:future_idx+1,stocks_hdr2idx['開盤價(元)']])[0])\n",
    "    close_price_array.append(list(company_stock_test_np[future_idx:future_idx+1,stocks_hdr2idx['收盤價(元)']])[0])\n",
    "    high_price_array.append(list(company_stock_test_np[future_idx:future_idx+1,stocks_hdr2idx['最高價(元)']])[0])\n",
    "    low_price_array.append(list(company_stock_test_np[future_idx:future_idx+1,stocks_hdr2idx['最低價(元)']])[0])\n",
    "\n",
    "    test_day_array.append(today_date)\n",
    "    actual_up_dn = up_down[future_idx]\n",
    "    company_doc = np.array(get_doc_by_date(list(np.array(today_date)), company_ctgry_np, ctgry_hdr2idx))\n",
    "\n",
    "    if (company_doc.size>2):\n",
    "        chi_all_x, chi_all_vec = get_count_tool(company_doc)\n",
    "        tokens = np.array(chi_all_vec.get_feature_names())\n",
    "        company_tf_np = np.zeros((company_doc.size,len(all_kw)))\n",
    "        for n, kw in enumerate(all_kw):\n",
    "            try:\n",
    "                idx = np.where(tokens==kw)[0][0]\n",
    "            except:\n",
    "                idx = -1\n",
    "\n",
    "            if (idx>=0):\n",
    "                company_tf_np[:,n] = chi_all_x.toarray()[:,idx]\n",
    "\n",
    "        fpredict = forest.predict(company_tf_np)\n",
    "        unique, counts = np.unique(fpredict, return_counts=True)\n",
    "        if unique.size==1:\n",
    "            p_up_down = unique[0]\n",
    "        else:\n",
    "            p_up_down = unique[1] if counts[1]>counts[0] else unique[0]\n",
    "            \n",
    "    ttl_doc_cnt.append(company_doc.size)\n",
    "    ttl_doc_ma_f.append(company_doc.size)\n",
    "    ttl_doc_ma_f.pop(0)\n",
    "    ttl_doc_ma_s.append(company_doc.size)\n",
    "    ttl_doc_ma_s.pop(0)\n",
    "    ttl_ma_f = np.mean(ttl_doc_ma_f)\n",
    "    ttl_ma_s = np.mean(ttl_doc_ma_s)\n",
    "    ttl_ma_cross_dly = ttl_ma_cross\n",
    "    if ttl_ma_f<ttl_ma_s:\n",
    "        ttl_ma_cross = 0\n",
    "    else:\n",
    "        ttl_ma_cross = 1\n",
    "    ttl_ma_cross_edge = ttl_ma_cross==1 and ttl_ma_cross_dly==0\n",
    "    \n",
    "    if company_doc.size<=2:\n",
    "        up_doc_cnt.append(0)\n",
    "        dn_doc_cnt.append(0)   \n",
    "    elif unique.size==1:\n",
    "        if unique[0]==dn_tick:\n",
    "            up_doc_cnt.append(0)\n",
    "            dn_doc_cnt.append(counts[0])\n",
    "        else:\n",
    "            up_doc_cnt.append(counts[0])\n",
    "            dn_doc_cnt.append(0)\n",
    "    else:\n",
    "        up_doc_cnt.append(counts[up_tick])\n",
    "        dn_doc_cnt.append(counts[dn_tick])\n",
    "    \n",
    "    if p_up_down == up_tick and up_doc_cnt[-1]>0:\n",
    "        portion = (up_doc_cnt[-1]-dn_doc_cnt[-1])/up_doc_cnt[-1]\n",
    "    elif p_up_down == dn_tick  and dn_doc_cnt[-1]>0:\n",
    "        portion = (dn_doc_cnt[-1]-up_doc_cnt[-1])/dn_doc_cnt[-1]\n",
    "    else:\n",
    "        portion = -1\n",
    "    if company_doc.size>5 and ttl_ma_cross_edge and ttl_ma_f-ttl_ma_s>0.04 and actual_up_dn!=no_tick:\n",
    "        print('Test Date= {}, DocCnt={}(U {} D {} P {:.2f}), Prediction={}, Acutual={}'.\n",
    "              format(today_date+timedelta(days=pre_day), company_doc.size, up_doc_cnt[-1], dn_doc_cnt[-1], portion,\n",
    "                     p_up_down, actual_up_dn) )\n",
    "        predict_y.append(p_up_down)\n",
    "        actual_y.append(actual_up_dn)\n",
    "ma20 = get_ma(close_price_array, 20)\n",
    "ma5 = get_ma(close_price_array, 5)\n",
    "accuracy = metrics.accuracy_score(actual_y, predict_y)\n",
    "precision = metrics.precision_score(actual_y, predict_y, average='micro')\n",
    "recall = metrics.recall_score(actual_y, predict_y, average='micro')\n",
    "f1 = metrics.f1_score(actual_y, predict_y, average='weighted')\n",
    "target_name = ['DOWN', 'UP']\n",
    "print(metrics.classification_report(actual_y, predict_y, target_names=target_name))\n",
    "print('{} Tested accuracy = {}'.format(company, accuracy))\n",
    "ttl_doc_cnt_ma1 = get_ma(ttl_doc_cnt, 5)\n",
    "ttl_doc_cnt_ma2 = get_ma(ttl_doc_cnt, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "font_set = FontProperties(fname=r\"c:\\windows\\fonts\\kaiu.ttf\", size=20)\n",
    "plt.title('DocCnts-'+company, fontproperties=font_set)\n",
    "ax = fig.add_subplot(3, 1, 1)\n",
    "ax2 = fig.add_subplot(3, 1, 2)\n",
    "ax3 = fig.add_subplot(3, 1, 3)\n",
    "ax2.plot(ttl_doc_cnt,color='olive', linewidth=4,  marker=\"s\", label='Total Docs')\n",
    "ax2.plot(up_doc_cnt,color='blue', linewidth=1,  marker=\"s\", label='Up Docs')\n",
    "ax2.plot(dn_doc_cnt,color='red', linewidth=1,  marker=\"s\", label='Down Docs')\n",
    "ax3.plot(np.concatenate(([np.NaN]*5,ttl_doc_cnt_ma1[5:len(ttl_doc_cnt_ma1)])),color='purple', linewidth=3, label='Docs MA_F')\n",
    "ax3.plot(np.concatenate(([np.NaN]*10,ttl_doc_cnt_ma2[10:len(ttl_doc_cnt_ma2)])),color='orchid', linewidth=3, label='Docs MA_S')\n",
    "ax.plot(np.concatenate(([np.NaN]*20,ma20[20:len(ma20)])),color='magenta', linewidth=2, label='MA20')\n",
    "ax.plot(np.concatenate(([np.NaN]*5,ma5[5:len(ma5)])),color='green', linewidth=2, label='MA5')\n",
    "ax.legend()\n",
    "ax2.legend()\n",
    "ax3.legend()\n",
    "new_date = []\n",
    "for dt in test_day_array:\n",
    "    new_date.append(dt[0].strftime(\"%Y/%m/%d\"))    \n",
    "ax.set_xticks(range(0, len(new_date), 10))\n",
    "ax.set_xticklabels(new_date[::10], rotation=30)\n",
    "ax3.set_xticks(range(0, len(new_date), 10))\n",
    "ax3.set_xticklabels(new_date[::10], rotation=30)\n",
    "mpf.candlestick2_ochl(ax, open_price_array, close_price_array, high_price_array, low_price_array,\n",
    "                     width=0.6, colorup='r', colordown='g', alpha=0.75); "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
